\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
% ARTISTO or ArtDet or ArtVisual
\title{ ArTection: an Art Detection Tool to Locate and Recognize Paintings and People in Museums and Art Galleries}

\author{Roberto Amoroso\\
University of Modena and Reggio Emilia\\
{\tt\small 219620@studenti.unimore.it}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   This work proposes a method to locate and recognize paintings and people in a museum or art gallery. For this purpose, I created a Python program that is able to locate and recognize paintings and people present in a video or single image. For the part relating to the paintings, I used the OpenCV library, while to carry out the people detection operation I used YOLO, a real-time object detection system.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Detect a painting, computing the transformation to rectify the image and then comparing the image obtained with those stored in a database, are all nontrivial tasks. Detect and analysing paintings is of course of great interest to art historians, and can help them to take full advantage of the massive databases that are built worldwide.

At the core of many recent computer vision works, the object detection task (classifying and localising an object) has
been less studied in the case of paintings.

In recent years, many works have been developed that investigate the problems of image detection \cite{fathy1995image,hambly2001supercosmos}, recognition \cite{martinel2013robust} and retrieval \cite{rui1999image}. 

Many of these approaches use Deep Learning techniques and are based on the use of Convolutional Neural Networks (CNNs) to carry out operations such as Painting Detection and Identification\cite{hong2019art}. This implies the need to have a large amount of annotated data, necessary to train and test the model.

The approach I propose avoids this problem by submitting the input image through a processing pipeline which, using the OpenCV \cite{bradski2008learning} library, performs a series of operations and transformations that produce the following results:
\begin{itemize}
   \item \textbf{Painting Detection}: detects all paintingds in the image.
   \item \textbf{Painting Segmentation}: creates a segmented version of the input, where the paintings, and also any statues, identified are white and the background is black.
   \item \textbf{Painting Rectification}: rectifies each painting detected, through an affine transformation.
   \item \textbf{Painting Retrieval}: matches each detected and rectified painting to a paintings DB, containing a list of the paintings in the museum or gallery with related information, such as title of the painting, author, room in which the painting is located. I used ORB \cite{rublee2011orb} as feature detector, to find keypoints and execute matching between an input image and the various database images.
   \item \textbf{People Detection}: detect people in the input using YOLOv3 \cite{redmon2018yolov3}, a state-of-the-art real-time object detection system and pre-trained weights.
   \item \textbf{People and Painting Localization}: locates paitning and people using information, using the information discovered during the painting retrieval phase.
\end{itemize} 

%------------------------------------------------------------------------
\section{Related Work}

\subsection{Object Detection}

One of the main problems in the field of computer vision is object detection. In the last few years, numerous works have been published to propose a possible solution capable of solving this task, leading to a continuous improvement in performance. A milestone, which led to great progress in this area, was the use of CNNs.

Pioneer in this sense was R-CNN (Regions with CNN features) \cite{girshick2014rich}, who had the idea of ​​using a selective search \cite{uijlings2013selective} image segmentation algorithm to generate many candidate regions for potential object instances before CNN is used to perform classification to these regions individually. Subsequently, other \cite{girshick2015fast, ren2015faster} works have unified the localization and classification phases in order to improve the speed of object detection.

In the wake of these pioneers, many other works have been conducted \cite{dai2016r, kim2016pvanet, lin2017feature, liu2016ssd, redmon2017yolo9000, redmon2016you, shrivastava2016training, redmon2018yolov3} aimed at further improving the performance of the architecture.

I decided to use a CNN-based object detector only to perform the people detection task. In particular, I selected YOLOv3 as it is a balanced system in terms of speed and accuracy, it comes with a well organized code and pre-trained weights.

%------------------------------------------------------------------------

\subsection{Image local features}
Paintings, statues and all objects present in a museum or art gallery can be filmed and photographed from various viewpoints and with different lighting conditions. This implies the need for a technique capable of representing an image as invariant to rotation, affine transformation, and some noise.

Hand-crafted image local features \cite{rublee2011orb,alahi2012freak,bay2006surf,jain2000statistical,leutenegger2011brisk,lowe2004distinctive,morel2009asift} can be used to solve these problems. These are techniques used for object tracking, image stitching, image registration, etc., i.e. all those applications that are based on finding correspondences between two images.

In this work, among the various image local features proposed, I selected ORB. It is a good alternative to SIFT and SURF in computation cost, matching performance and mainly it's not patented (SIFT and SURF are patented and you are supposed to pay them for its use). ORB is a good choice in low-power devices.

Here we have brief introductions to ORB, for more information and the details about how it works read the original paper \cite{rublee2011orb}.

ORB is a fusion of FAST keypoint detector and BRIEF descriptor with many modifications to enhance the performance. First it use FAST to find keypoints, then apply Harris corner measure to find top N points among them. It also use pyramid to produce multiscale-features.

ORB's main contributions are:
\begin{itemize}
   \item The addition of a fast and accurate orientation component to FAST.
   \item The efficient computation of oriented BRIEF features.
   \item Analysis of variance and correlation of oriented BRIEF features.
   \item A learning method for decorrelating BRIEF features under rotational invariance, leading to better performance in nearest-neighbor applications.
\end{itemize}
 

%------------------------------------------------------------------------
\section{Method}

%------------------------------------------------------------------------
\subsection{Material and Data Preparation}

The data available and representing the inputs of our program are:
\begin{itemize}
   \item A series of videos recorded inside the Estense Gallery in Modena, Italy.
   \item A database consisting of 95 images of as many paintings.
   \item A CSV file containing important information on each of the 95 paintings in the database, such as: title of the painting, author, room of the museum in which it is located, filename of the image.
\end{itemize}

The videos were recorded using different devices, angles and orientations, and during normal museum activity, so they often show people.

It was therefore necessary to create a program that was able to process not only individual images but also videos.
In the latter case, that is, when the input is a video, the solution I adopted is to consider each frame of the video as an image to submit to our processing pipeline.

The processing pipeline, therefore, works with images as input and produces in output, for each of them, another image, on which the obtained information has been reported (painting and people bounding boxes, title of the paintings, number of the room where the paintings and people are located).

The Painting Rectification task represents a particular case. In fact, it requires that for each painting present in an input image or video frames, an image containing the rectified version of the painting is saved as output. So in this case, against a single input there are potentially multiple outputs.

One of the first problems I faced concerns the different resolution of the videos, which are provided in the formats: HD (1280x720), full-HD (1920x1080) and 4K (3840 x 2160).

Having inputs with different resolutions posed the problem of making the measures adopted in the various functions of the processing pipeline (often expressed in pixels) invariant with respect to the resolution.

Our solution was to perform as a first operation of our processing pipeline a resize of all the inputs to the lower resolution, the HD one.

Subsequently, the resized images continue in the pipeline and are subjected to the various processing described below.

The information found (bounding boxes and information about paintings and room) is then subjected to an upscaling phase, i.e. the $(x, y)$ coordinates of contours, corners, bounding boxes are multiplied by the scale factor for which the original image has been resized to obtain its HD version.

The output is generated by drawing this information on the original image. This allows us to maintain the original resolution and improve performance as the functions of the processing pipeline are performed on tensors of a smaller size (e.g. a ninth of the input size in the case of 4K images or videos). 

Having obtained from our processing pipeline the various processed images, these are: directly stored in case the input is an image, treated as video frames and stored as a single video in case the input is a video.

%------------------------------------------------------------------------
\subsection{Painting Detection and Segmentation}

To aid the description of the processing of locating, rectifing and recognising a painting in an image, let's suppose we want to recognize the painting 'TODO', represented in Figure \ref{fig:dbPainting}, inside the image present in Figure \ref{fig:originalImage}.

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{'TODO', the painting we want to locate, rectify and recognize}
   \label{fig:dbPainting}
   \end{figure}

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{The input image of our processing pipeline.}
   \label{fig:originalImage}
\end{figure}

%------------------------------------------------------------------------
\subsubsection{Obtain a Wall Mask}

Inside a museum or art gallery the paintings are objects hanging on the walls. These are assumed to be of a single uniform color, as is usually the case in museums and art galleries in order to avoid distracting visitors from artwork.

On the basis of this assumption, the first operation to be carried out to identify and segment the paintings is to distinguish which portions of the image are not walls. 

To achieve this, a sequence of operations must be performed. First I apply the the Mean Shift Segmentation operation on the image, which clusters nearby pixels with similar pixel values and sets them all to have the value of the local maximum of pixels value. The result is to have the pixels grouped by color and location, as can be seen from the image in Figure \ref{fig:meanShiftSegmentation}

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{Result of the mean-shift-segmentation on the input image.}
   \label{fig:meanShiftSegmentation}
\end{figure}

On the resulting image I perform a second operation, called Flood Fill, which assigns the same color to connected components.

The connectivity is determined by the color closeness of the neighbor pixels. That is, to be added to the connected component, the color of the pixel should be close enough to color of one of its neighbors that already belong to the connected component.

I assign the connected components the value 255 (white) and set all remaining pixels to 0 (black). In this way, for each Flood Fill operation performed, a mask is obtained in which we can find regions of 2 possible colors, white or black. This process is repeated on the pixels of the image in order to identify the mask in which the white region is the largest and I assume that this region is the wall and that the black regions are the paintings.

The assumption for which the largest region is considered to be the wall, although proving valid in most cases, presents problems when the image contains a very large or very close painting. In this case, the assumption is no longer valid as the largest region will be the one associated with the painting, which will be white, while the wall will be colored black.

Obviously, since this problem occurs right at the beginning of the pipeline, it would cause a chain of incorrect predictions. For this reason, I found a solution to solve the problem. Before proceeding with its description, it is necessary to have subjected the image to a series of other operations.

First, the colors of the mask are inverted, assigning 255 to the pixels having value 0 and vice versa. The result is a mask in which the wall is black and the paintings white, called the \textit{wall mask}. To remove any noise present inside the mask, it is first eroded and then dilated equally.

Erosion involves moving a kernel over the pixels of a binary image. A pixel in the original image (either 1 or 0) will be considered 1 only if all the pixels under the kernel is 1, otherwise it is eroded (made to zero).

Dilation is the opposite of erosion. Dilation involves moving a kernel (a matrix of a given size) over the pixels of a binary image. When the kernel is centered on a pixel with a value of 0 and some of its pixels are on pixels with a value of 1, the center pixel is given a value of 1.

The result of the flooding operations and the wall mask obtained after inversion, erosion and dilatation can be seen in the Figure \ref{fig:maskLargestSegmentInveted} 

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{Mask showing the result of the flood fill operation (left) and the wall mask obtained after inversion, erosion and dilatation (right).}
   \label{fig:maskLargestSegmentInveted}
\end{figure}

%------------------------------------------------------------------------
\subsubsection{Connected Component Analysis}

The Figure \ref{fig:maskLargestSegmentInveted} shows that not all the white components are paintings, for example even the plaques containing the description of the painting are considered to be paintings themselves. Also any portions of the ceiling or floor in the image could be considered paintings.

To solve these problems I introduced a selection phase, consisting of some criteria that each component must respect in order to be considered a painting:
\begin{itemize}
   \item it should not span the entire image.
   \item their bounding boxes devono avere un'area minima di 150x150 pixels. This value allowed me to eliminate small objects such as descriptive targe or small fire extinguishers.
   \item the area of ​​the painting must occupy at least 60\% of its bounding boxing. That means that the paintings are also assumed to be somewhat rectangular. This assumption helps to eliminate parts of the wall and ceiling that may be in the input image.
\end{itemize}

If some paintings do not fall under these criteria, they are not categorised as paintings. This could occur for example for very small paintings or pictures of paintings that have been taken very far away from the painting.

To determine whether or not a component meets these criteria, the contours of all the white objects in the wall mask are found. For each of them I create a rotated rectangle of the minimum area enclosing the contours 2D point set.
The area of ​​the contours is considered as the area of ​​the paintings, and all the contours that do not comply with the previous criteria are discarded.

At this point we have a list of outlines each associated with a painting.

%------------------------------------------------------------------------
\subsubsection{Refine the Wall Mask}

Here comes my solution to the problem that occurs when flood fill considers the wall as paintings and vice versa.

The solution I have adopted can be summarized as follows:
\begin{enumerate}
   \item I consider the wall mask.
   \item I apply a one-pixel white border along the outline of the mask.
   \item Calculate all contours of the white components again.
   \item I get two sets of contours, the first associated with the wall mask without white border, the second associated with the wall mask having the white border.
   \item I consider the set of contours with the largest number of elements. In case they have equal size, I choose the second.
   \item If the selected contour set is the first, then I perform the contour selection phase as described above. If the set of contours selected is the second, then I am in the situation where the flood fill has identified the paintings as a wall and vice versa. Then I invert the wall mask again, so as to bring back to the correct situation in which the paintings are white and the wall black, and perform the contour selection phas, adding a new control at the end. It consists in eliminating all the contours enclosing other contours within them.
\end{enumerate}

To understand why my solution works, let's consider the possible situations that can occur:
\begin{itemize}
   \item the wall mask found with the flood fill is wrong and the addition of the white border determines the identification of a number of contours equal to the case of wall mask without white border. We are necessarily in case the flood fill worked wrong, because if the wall (background of the paintings) had been correctly black we would certainly have found at least one more contour, the one added to the wall mask. This situation also occurs when the incorrectly identified paintings (i.e. black regions in the wall mask) are in direct contact with the edges of the image. My solution is also able to manage this case, because the addition of the white border separates the paintings from the edge and allows me to correctly identify their contours.
   
   \item the wall mask found with the flood fill is wrong and the addition of the white border determines the identification of a greater number of contours than in the case of wall mask without white border. This situation occurs when the flood fill function  worked incorrectly and we have paintings (in this case black regions of the mask) that touch the edges of the image. Exactly as said for the previous case, my solution is able to detect this error and correct it.

   \item the wall mask found with the flood fill is correct and the addition of the white border determines the identification of a single new contour around the whole image. The number of contours will be one greater than in the case of wall mask without the white border. This is not a problem because this contour will be eliminated in the phase of contours selection, as large as the whole image. Excluding the border contour, the other contours identified within the image (those of the paintings) will remain unchanged.
   
   \item the wall mask found with the flood fill is correct and the addition of the white border decreases the number of contours found. We are in the case in which the paintings (in this case white regions of the mask) are only partially inside the image and/or touch the edge of the same. In this case, I correctly consider the contours found with the wall mask without white border, because in higher numbers. 
\end{itemize}


An example of an incorrect wall mask where the color of wall and painting are inverted, and the correct version obtained applying my solution can be seen in the Figure \ref{fig:wallMaskCorrection}


\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{Incorrect wall mask where the color of wall and painting are inverted (left) and the correct version obtained by applying the solution proposed (right).}
   \label{fig:wallMaskCorrection}
\end{figure}


The additional control, which eliminates all the contours that enclose other contours within them, is necessary to prevent that the addition of the white border combined with disturbing objects in the image (e.g. a railing protecting a painting) from generating unwanted contours that contain one or more paintings inside. Obviously, I am assuming that it is not possible to have a painting within another painting, a condition that can almost always be considered verified and that in particular it is with regard to the videos and images that make up the database.

%------------------------------------------------------------------------
\subsubsection{Painting Segmentation}

At this point, I obtained a list of the contours of the paintings, including the relative frames, which are inside the image.

To segment the original image, I just need to generate a completely black image with the same resolution and draw on it only the contours that have passed all the previous tests, filling them in white. The paintings, including their frames, will be the only white components of the image. 

The tests carried out have shown that this method is also capable of identifying and segmenting any statues present in the image, if sufficiently large. 

The result of the segmentation can be observed in Figure \ref{fig:imageSegmented}

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{Segmented version of the input image.}
   \label{fig:imageSegmented}
\end{figure}



The Painting detection and Segmentation task therefore appears to have been completed. In reality, the result thus obtained is limited to drawing rectangular bounding boxes around the outline of the paintings, as shown in Figure \ref{fig:imageRectROI}.

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{Image showing the rectangular bounding box of the paintings.}
   \label{fig:imageRectROI}
\end{figure}

The result that I would like to achieve is to build a Region of Interest (ROI) that fits the contour of the painting as best as possible, which due to distortions related for example to perspective, may not be exactly rectangular, especially when positioned not exactly in front of the device that took the photo or recorded the video.

To improve the detection result, a series of additional steps and transformations are necessary, which are also propaedeutics
to the execution of the next task, the Painting Rectification.

%------------------------------------------------------------------------
\subsection{Painting Rectification}

\subsubsection{Find Corners}

The starting point is the previously constructed segmented image, which will now be our mask, and a list of the contours of the paintings detected in the image with their bounding boxes.

In this phase, the previously identified paintings are considered also including the frame. It is therefore necessary to erode the mask, in order to remove a large part of the frame and isolate only the painting. After erosion, the white components of the image are the paintings, without the frame.

Let's now consider each of these components individually, that is, each of the detected paintings. To isolate a component just consider only the portion of the original image and the mask enclosed by the bounding box of the component itself. We obtain what I have defined sum-image and sub-mask respectively. An example is present in Figure \ref{fig:subImageMask}

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{Sub-image (left) and sub-mask (right) of a painting component.}
   \label{fig:subImageMask}
\end{figure}

I now describe the sequence of operations necessary to find the corners of the painting and apply the affine transformation that allows to rectify the painting and be ready for the next phase, the Painting Retrieval.

First we identify the lines that represent the edges of the painting components. To do this, a Median Filter is applied to the components in order to smooth the outline. The median filter run through each pixel of an image and replace its value with the median of its neighboring pixels (located in a square neighborhood around the evaluated pixel). 

The result of the erosion and smoothing operations can be seen in Figure \ref{fig:erosionAndMedianFilter}

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{Result of component erosion e smoothing using a Median Filter.}
   \label{fig:erosionAndMedianFilter}
\end{figure}

In order to apply the Hough Line Transform to detect straight lines representing the edges of the painting, we need to build an image in which only the edges of the painting are present.

To do this, run the Canny Edge Detection \cite{canny1986computational} algorithm on the smoothed sub-mask to obtain an edge image. 

Canny edge detection examines the rate of change of brightness values in pixels in versions of the image to which a 5x5 Gaussian Filter has been applied to remove the noise. Gaussian Filter involves setting the value of each pixel on a weighted average of the values of the neighboring pixel. In Canny Edge Detection, each pixel is checked to see how much the brightness changes from side to side for many orientations and in many gaussian filtered versions of the image. Since the edges can be represented as a gradual change in brightness over many pixels, Canny uses local maxima

On the edge image is applied the Hough Tranform \cite{ballard1987generalizing} to detect straight lines. 

Lines can be represented as an orthogonal distance to the origin and an angle of the line with respect to an axis. In Hough Lines, an "accumulator" of "cells" is created that represents different combinations of distance and angle. For each point on the edge of the image, all cells representing a line that crosses that point are incremented. Then the local maximum of the accumulator are sought.

From the lines obtained, a mask is created starting from a black image having the same size as the sub mask. Each line found is drawn in white and with a certain tickness on this mask, with a length such as to cross the whole mask. This operation brings with it another advantage. In fact, the tickness with which the lines are drawn allows us to remove other possible frame pixels from the painting component and to merge multiple lines that represent the same edge of the painting.

The result of the edge detection operations and the mask obtained from the Hough lines can be seen in the Figure \ref{fig:cannyHough}.

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{Result of Canny Edge detection (left) and the mask obtained from Hough Lines (right).}
   \label{fig:cannyHough}
\end{figure}


The mask built using the Hough lines allows us to isolate the painting component with a few simple operations. First of all find all the contours inside the mask. Of these I consider the contours that encloses the largest area, which represents the painting I am looking for.

I draw this outline on a black mask of the same size as the sub-mask and fill it in white. So I isolated the painting component.

The painting component mask is used to identify the corners necessary to transform the detected painting before comparing it with the images in the database.

The program uses the Shi-Tomasi Corner Detector \cite{shi1994good}. It shows better results compared to Harris Corner Detector \cite{harris1988combined} and allows me to identify the strongest $N$ corners in the image. In my case, I placed $N = 4$, as interested in finding the four corners of a painting. Obviously, I am assuming that the paintings are all somewhat rectangular and therefore it is always possible to obtain four corners. In reality, paintings can have more disparate shapes and have a number of corners greater than or less than 4.

To solve this problem I perform a check phase on the corners found by the Shi-Tomasi Corner Detector. In fact, these are considered valid only if they meet the following conditions:
\begin{itemize}
   \item are exactly 4
   \item the area of ​​the polygon having as vertices the identified corners must be greater than or equal to a certain threshold obtained as a percentage of the area of ​​the contour of the painting component.
\end{itemize}

If the above conditions are not respected, the four vertices (top-left, top-right, bottom-right, bottom-left) of the sub-image are considered as corners of the painting component.

This allows me to manage without errors even non-rectangular images. The drawback of this solution is that, if the previous conditions are not respected, after setting the vertices of the sub-image as corners, the next phase of rectification will leave the painting components unchanged.

Figure \ref{fig:cornersDetect} shows a mask in which the correctly identified corners have been drawn.

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{Corners found using the Shi-Tomasi Corner Detector.}
   \label{fig:cornersDetect}
\end{figure}

%------------------------------------------------------------------------
\subsubsection{Rectify Painting}

We have all the necessary ingredients to be able to rectify the identified painting.

In fact, in order to compare the features of detected painting with those of the paintings in the database, it is advisable to carry out an Affine Transformation that uses the previously found corners.

This Affine Transformation is done by translating every pixel in an image to a new location. The transformation is defined by a matrix multiplication, that can be found when the translation of some points are known. So to obtain this matrix, we need to know a set of source points and a set of destination points.

The source points are the previously found corners. As for the destination points, these are determined in the following way:
\begin{enumerate}
   \item The corners are sorted according to the top-left, top-right, bottom-right, and bottom-left order.
   
   \item I calculate the width $w$ of the new image, which will be the maximum distance between bottom-right and bottom-left x-coordiates or the top-right and top-left x-coordinates
   
   \item I calculate the height $h$ of the new image, which will be the maximum distance between the top-right and bottom-right y-coordinates or the top-left and bottom-left y-coordinates. 
   
   \item Now that I have the dimensions of the new image, I construct the set of destination points ($dst\ _points$) to obtain a top-down view of the image, again specifying points in the top-left, top-right, bottom-right, and bottom-left order:
   \begin{equation}
      dst\_points = [[0, 0], [w, 0], [w, h], [0, h]]
      \label{eq:1}
   \end{equation}
\end{enumerate}


Figura \ref{fig:rectifyComparison} shows a comparison between the original image of a painting and its rectified version.

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{Original painting image (left) and its reftified version (right).}
   \label{fig:rectifyComparison}
\end{figure}

The program accept as a target for the affine transformation, not only a set of points but also an entire image. In this case the program translates the corners of the painting component identified to the four vertices of the destination image, always following the top-left, top-right, bottom-right, and bottom-left order. 

This additional functionality is useful for performing the next task,the Painting Retrieval.

%------------------------------------------------------------------------
\subsubsection{Improve Paintings ROI}

Before proceeding to the next task, let's briefly consider the Painting Detection task again. As previously said, the bounding box that is associated with each detected painting is represented so far by a rectangle. Now, however, I have the ingredient I need to build ROIs that best fit the outline of the painting, respecting the perspective. This ingredient is precisely the corners. By drawing a straight line that connects the various corners, I am in fact able to build a polygon that constitutes the ROI I was looking for. In Figure \ref{fig: improvedROI} you can see the result obtained, and in particular you can appreciate how it is better than the previous result based on rectangular bounding boxes.

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{ROI of the detected paintings obtained using the paintings corners.}
   \label{fig:improvedROI}
\end{figure}


%------------------------------------------------------------------------
\subsection{Painting Retieval}

At this stage, we have a rectified version of a painting detected within the original image. All that remains is to compare it with all the paintings in the database in order to find the one that generates the best match.

Lighting problems, light reflection, image exposure and image saturation due to the exposure of paintings in exhibitions, however, could negatively influence the chances of correctly identifying a painting among those present in the database.

For these reasons, before using ORB to match the paintings, it is advisable to try to improve the brightness and contrast of the image.

To do this, the program is able to automatically adjust the brightness and construction of the image. Brightness and contrast are linear operator with parameter $\alpha$ and $\beta$:
\begin{equation} 
   g(x,y)= \alpha * f(x,y)+ \beta 
   \label{eq:2}
\end{equation}

The question is: How to automatically calculate $\alpha$ and $\beta$?

To do this, we can look at the histogram of the image. Automatic brightness and contrast optimization calculates $\alpha$ and $\beta$ so that the output range is [0...255]. 

We calculate the grayscale histogram of the image, and use it to calculate a cumulative distribution, necessary to determine where color frequency is less than some threshold value (tipically 1\%) and cut the right and left sides of the histogram. This gives us our minimum ($min\_gray$) and maximum ($max\_gray$) grayscale ranges.

To calculate $\alpha$, we take the minimum and maximum grayscale range after clipping and divide it from our desired output range of 255:
\begin{equation} 
   \alpha = 255 / (max\_gray - min\_gray)
   \label{eq:3}
\end{equation}

To calculate $\beta$, we plug it into the formula \ref{eq:2} where $g(x, y)=0$ and $f(x, y)=min\_gray$. After solving we obatin:
\begin{equation} 
   \beta = -min\_gray * \alpha
   \label{eq:4}
\end{equation}

Once the auto-adjusted version of the rectified image has been obtained, the matching with the database is carried out. The program uses ORB to locate the keypoints in the paintings, and calculates how many matches the auto-adjusted painting has produced with each painting in the database.

The key points are calculated for each pair of images you are comparing and the program checks which keypoints are shared between the two images. 

In order to reduce the number of false positives, i.e. matches with incorrect paintings, the program does not just select the database painting that produced the highest number of matches, but determines the best match in the following way:
\begin{enumerate}
   \item For each painting in the database, calculate the matches with the painting component that is being analyzed. The matches obtained are sorted in ascending order based on the distance (the first are the best matches). Of these, it considers the first $N$ (settable parameter) and calculates the average value, i.e. the average distance. If no match has been recorded, a value is returned which acts as a flag and which allows to understand that there was no match (i.e. a very high value).
   
   \item Create a list with the average distance recorded for each painting in the database and order it in increasing order.
   
   \item If the first value of the list, the lower one, is the flag value, then it means that there was no match with the database.
   
   \item If the first value of the list is different from the flag value then this match is considered valid only if the ratio between the first value of the list and the second value of the list is lower than a certain threshold. That is, I want the first value, the one that determines which painting of the database is assigned to the painting component, is sufficiently smaller than the second value in the list. If this is verified then I can consider the first match of the list sufficiently robust and reliable, otherwise it will mean that the match is weak and not very reliable and I drop it.
\end{enumerate}

In the event that ORB has not produced any match with any of the paintings in the database for a given detected painting, the program also offers the possibility to perform a histogram comparison for each painting saved in the database, in order to identify which painting has the histogram more similar to that of the detected painting. The painting with the best matching histogram is chosen as a painting to classify the detected painting

Histograms are made for the Hue and Saturation of each image. This involves making 'bins' for each possible Hue or Saturation value in the image, counting up the amount of pixels in the image that have that Hue or Saturation value. The images' histograms are then compared to see which of the saved painting images has the the most similar histogram to the histogram of the located painting.  

To compare two histograms ($H_1$ and $H_2$), first we choose the histogram Intersection as a metric ($d(H_1, H_2)$) to express how well both histograms match:
\begin{equation}
   d(H_1, H_2) = \sum_{I} min(H_1(I), H_2(I))
   \label{eq:5}
\end{equation}

In the case of histogram matching, the match that produced the highest intersection value is considered the best match.

Suppose we were able to find a match with the database. We can now access all the information associated with the painting such as title, author, filename and room in which it is located.

All very useful information that are necessary to perform the Painting and People Localization task.

%------------------------------------------------------------------------
\subsection{People Detection}

This task is performed independently from the previous ones also because it uses a different approach. In fact, it uses the real-time object detection YOLOv3 to identify any people present in the image.

YOLOv3 outputs a list of bounding boxes each of which associated with a person identified in the image.

However, we are inside a museum or art gallery, many paintings portray people, so it often happens that YOLO identifies the subjects of the paintings as people. This, although logically correct, is not the result we want to achieve.

To solve this problem, I consider unacceptable and consequently rejected all bounding boxes which overlap more than a fixed threshold with any one of bounding boxes of the detected paintings.

Figure \ref{fig:peopleDetection} shows an example of the result of the People Detection operation performed on an image. It should be noted that although the person covers a portion of the painting with his body, both the painting and the person are correctly identified.

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{Example of output of the people detection task.}
   \label{fig:peopleDetection}
\end{figure}

%------------------------------------------------------------------------
\subsection{Painting and People Localization}

The last step of the pipeline is the Painting and People Localization task. It aims to determine which room in the museum the paintings and people in the input image are located in.

By taking advantage of the information we got to reach this point, this task can be solved with a trivial but effective solution.

The basic idea is, the people and paintings in the image are in the same room. So, if I am able to determine in which room the paintings in the image are located, I have consequently also identified where the people are.

This assumption may not be verified in the event that the image contains portions of different rooms, because perhaps taken by including a door or access to an adjacent room.
However, these are particular cases, which do not occur too frequently within the available dataset.

For each of the paintings recognized in the image, thanks to the information obtained from the database during the painting retireval phase, the program is able to determine in which museum room it is located.

The program creates a list of the rooms of all the paintings identified within the image. Due to incorrect identification of the paintings, it may happen that the list contains different values.

The program counts how many times each room number is present in the list and considers the one that has the most occurrences. It is a majority decision. If most of the paintings detected are in a particular room, then it is very likely that it is the correct room.

Figure \ref{fig:paintingPeopleLocalization} shows the result of the Painting and People Localization process, which consists in superimposing the information of the room where paintings and people are located on the original image.

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{Example of output of the Painting and People Localization task.}
   \label{fig:paintingPeopleLocalization}
\end{figure}

%------------------------------------------------------------------------
\section{Results}

\begin{figure*}
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
   \end{center}
      \caption{Example of final output produced by the proposed pipeline.}
   \label{fig:finalPipelineOutput}
\end{figure*}

The tests carried out have shown that the program performs well for most of the data in the dataset. An example of complete output showing the final result produced by the pipeline can be seen in Figure \ref{fig:finalPipelineOutput}.

The results obtained are great although the system has some difficulties in dealing with particularly difficult situations. This is due to the fact that common image processing techniques suffer from some problems (lighting, scale changes, distortion, etc.) which become even more relevant in the case of moving cameras, which in turn introduce further undesirable effects, e.g. blur, noise, motion.

In addition, the exhibition of paintings in museums and art galleries, adds problems such as light reflection, image exposure and image saturation, and involves the presence of people who overlap, to a greater or lesser extent, with the paintings.

Although the problem of brightness and contrast have been solved using auto-adjusting techniques, there are some problems that are instead related to the assumptions made during the implementation of the various tasks.

For example, the program may work not correctly when the frames of the paintings overlap, when the paintings are too small or taken from afar, when the frame is particularly wide. The latter case could adversely affect the result as the erosion phase could not be able to remove enough frame, which could compromise the features matching or histogram comparison phase, making the result less accurate.

However, the program is able to correctly manage situations in which the painting is very close or is very large and occupies the largest portion of the image (i.e. the flood fill operation mistake the wall and paintings in the Painting Detection phase) and the situation in which YOLO identifies as persons the subjects portrayed within the paintings.

A very important factor to highlight is that the various functions of the OpenCV library that have been used to implement the various tasks, often require a large number of parameters as input. This overall determines a very large number of possible combinations of these parameters. Each of these combinations determines a different set-up of the program and therefore different final results.
Again, numerous tests were carried out in order to find the right trade-off, that is the combination that produced the best results in the various presentable situations.

This, at the same time, opens the way to many other tests that can still be done in order to improve the results obtained with this work.

%------------------------------------------------------------------------
\section{Conclusions}

This work has shown how it is possible to complete the Painting Detection, Retrieval and Localization tasks without the use of deep learning techniques, but through operations and transformations of image processing and analysis.

I was able to face many of the challenges presented to me, such as the problem of incorrect flooding that recognized the wall and paintings in an inverted way, the problem of people detection that identified the subjects within the paintings or the contrast and lighting problems of the images, solved through auto-adjusting techniques that also allowed me to improve the match results with the database.

This work presents great results but also opens the way to a series of new possibilities and alternative solutions and different combinations of the operations carried out in the proposed pipeline, which bode well in the possibility that future works will be able to obtain an improvement in results and performance.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
